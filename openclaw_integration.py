#!/usr/bin/env python3
"""
CHATTY OpenClaw Integration
Autonomous learning and self-repairing system with multi-LLM orchestration and file chunking
"""

import os
import json
import asyncio
import logging
import hashlib
import re
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
from collections import defaultdict

logger = logging.getLogger(__name__)

# â”€â”€ Optional heavy dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from sentence_transformers import SentenceTransformer
    _SENTENCE_TRANSFORMER_AVAILABLE = True
except ImportError:
    _SENTENCE_TRANSFORMER_AVAILABLE = False

try:
    import networkx as nx
    _NETWORKX_AVAILABLE = True
except ImportError:
    _NETWORKX_AVAILABLE = False

try:
    import numpy as np
    _NUMPY_AVAILABLE = True
except ImportError:
    _NUMPY_AVAILABLE = False


# â”€â”€ File-based memory replacement â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class _FileMemory:
    """Minimal file-based replacement for the old memory_system dependency."""

    def __init__(self, path: Path):
        self._path = path
        self._path.parent.mkdir(parents=True, exist_ok=True)

    def _load(self) -> List[Dict[str, Any]]:
        if not self._path.exists():
            return []
        try:
            return json.loads(self._path.read_text(encoding="utf-8"))
        except (json.JSONDecodeError, ValueError):
            return []

    def _save(self, data: List[Dict[str, Any]]):
        self._path.write_text(json.dumps(data[-500:], indent=2, ensure_ascii=False), encoding="utf-8")

    def store_experience(self, category: str, content: Dict[str, Any]):
        entries = self._load()
        entries.append({
            "type": category,
            "content": content,
            "timestamp": datetime.utcnow().isoformat(),
        })
        self._save(entries)

    def get_recent_experiences(self, limit: int = 10, experience_type: str = None) -> List[Dict[str, Any]]:
        entries = self._load()
        if experience_type:
            entries = [e for e in entries if e.get("type") == experience_type]
        return entries[-limit:]


_memory = _FileMemory(Path("generated_content") / "openclaw_memory.json")


# â”€â”€ FileChunker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class FileChunker:
    """OpenClaw-style file chunking and context management"""

    def __init__(self):
        self.embeddings_model = None
        if _SENTENCE_TRANSFORMER_AVAILABLE:
            try:
                self.embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')
            except Exception:
                logger.warning("Sentence transformer model failed to load â€” chunking will work without embeddings")

        self.chunk_cache: Dict[str, List[Dict[str, Any]]] = {}
        self.context_graph = nx.DiGraph() if _NETWORKX_AVAILABLE else None

    def chunk_file(self, file_path: str, max_chunk_size: int = 2000) -> List[Dict[str, Any]]:
        """Intelligently chunk files based on semantic boundaries"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()

            file_type = Path(file_path).suffix.lower()

            if file_type in ['.py', '.js', '.ts', '.java', '.cpp', '.c']:
                chunks = self._chunk_code_file(content, file_path, max_chunk_size)
            elif file_type in ['.md', '.txt', '.rst']:
                chunks = self._chunk_text_file(content, file_path, max_chunk_size)
            else:
                chunks = self._chunk_generic_file(content, file_path, max_chunk_size)

            if self.context_graph is not None:
                self._build_context_graph(chunks, file_path)

            self.chunk_cache[file_path] = chunks
            logger.info(f"Chunked {file_path} into {len(chunks)} chunks")
            return chunks

        except Exception as e:
            logger.error(f"Failed to chunk file {file_path}: {e}")
            return []

    # â”€â”€ code chunking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _chunk_code_file(self, content: str, file_path: str, max_size: int) -> List[Dict[str, Any]]:
        chunks: List[Dict[str, Any]] = []
        current_chunk: Dict[str, Any] = {
            'content': '', 'start_line': 0, 'end_line': 0,
            'type': 'code', 'semantic_boundaries': [],
        }
        lines = content.split('\n')
        current_size = 0
        chunk_id = 0

        for i, line in enumerate(lines):
            line_size = len(line)
            boundary_type = self._detect_code_boundary(line, i, lines)

            if boundary_type and current_size > max_size * 0.7:
                if current_chunk['content']:
                    chunks.append(self._finalize_chunk(current_chunk, chunk_id, file_path))
                    chunk_id += 1
                current_chunk = {
                    'content': line + '\n', 'start_line': i, 'end_line': i,
                    'type': 'code', 'semantic_boundaries': [boundary_type],
                }
                current_size = line_size
            else:
                current_chunk['content'] += line + '\n'
                current_chunk['end_line'] = i
                current_size += line_size

            if current_size >= max_size:
                chunks.append(self._finalize_chunk(current_chunk, chunk_id, file_path))
                chunk_id += 1
                current_chunk = {
                    'content': '', 'start_line': i + 1, 'end_line': i + 1,
                    'type': 'code', 'semantic_boundaries': [],
                }
                current_size = 0

        if current_chunk['content']:
            chunks.append(self._finalize_chunk(current_chunk, chunk_id, file_path))
        return chunks

    def _detect_code_boundary(self, line: str, line_num: int, lines: List[str]) -> Optional[str]:
        stripped = line.strip()
        if re.match(r'^\s*(def|function|async\s+function)\s+\w+', stripped):
            return 'function_definition'
        if re.match(r'^\s*(class|interface|type)\s+\w+', stripped):
            return 'class_definition'
        if re.match(r'^\s*(import|from|require)\s+', stripped):
            return 'import_statement'
        if stripped.startswith('#') or stripped.startswith('//') or stripped.startswith('/*'):
            return 'comment_boundary'
        if not stripped and line_num > 0:
            prev_line = lines[line_num - 1].strip()
            if prev_line and not prev_line.startswith('#') and not prev_line.startswith('//'):
                return 'logical_boundary'
        return None

    # â”€â”€ text chunking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _chunk_text_file(self, content: str, file_path: str, max_size: int) -> List[Dict[str, Any]]:
        chunks: List[Dict[str, Any]] = []
        paragraphs = content.split('\n\n')
        current_chunk: Dict[str, Any] = {
            'content': '', 'start_paragraph': 0, 'end_paragraph': 0,
            'type': 'text', 'semantic_boundaries': [], 'start_line': 0, 'end_line': 0,
        }
        current_size = 0
        chunk_id = 0

        for i, paragraph in enumerate(paragraphs):
            para_size = len(paragraph)
            if paragraph.startswith('#') and current_size > max_size * 0.8 and current_chunk['content']:
                chunks.append(self._finalize_chunk(current_chunk, chunk_id, file_path))
                chunk_id += 1
                current_chunk = {
                    'content': paragraph + '\n\n', 'start_paragraph': i, 'end_paragraph': i,
                    'type': 'text', 'semantic_boundaries': ['heading'], 'start_line': 0, 'end_line': 0,
                }
                current_size = para_size
                continue

            if current_size + para_size > max_size:
                if current_chunk['content']:
                    chunks.append(self._finalize_chunk(current_chunk, chunk_id, file_path))
                    chunk_id += 1
                current_chunk = {
                    'content': paragraph + '\n\n', 'start_paragraph': i, 'end_paragraph': i,
                    'type': 'text', 'semantic_boundaries': [], 'start_line': 0, 'end_line': 0,
                }
                current_size = para_size
            else:
                current_chunk['content'] += paragraph + '\n\n'
                current_chunk['end_paragraph'] = i
                current_size += para_size

        if current_chunk['content']:
            chunks.append(self._finalize_chunk(current_chunk, chunk_id, file_path))
        return chunks

    # â”€â”€ generic chunking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _chunk_generic_file(self, content: str, file_path: str, max_size: int) -> List[Dict[str, Any]]:
        chunks: List[Dict[str, Any]] = []
        lines = content.split('\n')
        step = max(1, max_size // 100)
        for i in range(0, len(lines), step):
            chunk_lines = lines[i:i + step]
            chunks.append({
                'content': '\n'.join(chunk_lines),
                'start_line': i,
                'end_line': min(i + step - 1, len(lines) - 1),
                'type': 'generic',
                'semantic_boundaries': [],
            })
        return chunks

    # â”€â”€ finalize / graph â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _finalize_chunk(self, chunk_data: Dict[str, Any], chunk_id: int, file_path: str) -> Dict[str, Any]:
        chunk_id_str = f"{file_path}#{chunk_id}"
        embedding = None
        if self.embeddings_model:
            try:
                embedding = self.embeddings_model.encode(chunk_data['content']).tolist()
            except Exception as e:
                logger.warning(f"Embedding failed for {chunk_id_str}: {e}")

        return {
            'id': chunk_id_str,
            'file_path': file_path,
            'content': chunk_data['content'],
            'start_line': chunk_data.get('start_line', 0),
            'end_line': chunk_data.get('end_line', 0),
            'type': chunk_data['type'],
            'semantic_boundaries': chunk_data['semantic_boundaries'],
            'embedding': embedding,
            'size': len(chunk_data['content']),
            'created_at': datetime.utcnow().isoformat(),
        }

    def _build_context_graph(self, chunks: List[Dict[str, Any]], file_path: str):
        if self.context_graph is None:
            return
        for i, chunk in enumerate(chunks):
            cid = chunk['id']
            self.context_graph.add_node(cid, file_path=file_path, type=chunk['type'],
                                        size=chunk['size'], boundaries=chunk['semantic_boundaries'])
            if i > 0:
                self.context_graph.add_edge(chunks[i - 1]['id'], cid, relationship='sequential')

    def _calculate_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        if not _NUMPY_AVAILABLE:
            return 0.0
        try:
            v1, v2 = np.array(vec1), np.array(vec2)
            return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))
        except Exception:
            return 0.0

    def get_relevant_chunks(self, query: str, file_path: str = None, top_k: int = 5) -> List[Dict[str, Any]]:
        if not self.embeddings_model:
            return []
        try:
            query_embedding = self.embeddings_model.encode(query)
            target_chunks: List[Dict[str, Any]] = []
            if file_path and file_path in self.chunk_cache:
                target_chunks = self.chunk_cache[file_path]
            elif file_path is None:
                for chunks in self.chunk_cache.values():
                    target_chunks.extend(chunks)
            else:
                return []

            scored = []
            for chunk in target_chunks:
                if chunk.get('embedding'):
                    sim = self._calculate_similarity(query_embedding, chunk['embedding'])
                    scored.append((chunk, sim))
            scored.sort(key=lambda x: x[1], reverse=True)
            return [c for c, _ in scored[:top_k]]
        except Exception as e:
            logger.error(f"Relevant chunk lookup failed: {e}")
            return []


# â”€â”€ MultiLLMRouter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class MultiLLMRouter:
    """OpenClaw-style multi-LLM orchestration â€” delegates to revenue engine when available."""

    def __init__(self, revenue_engine=None):
        self.revenue_engine = revenue_engine
        self.task_router = TaskRouter()
        self.response_aggregator = ResponseAggregator()

    def route_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        try:
            task_analysis = self.task_router.analyze_task(task)
            selected_llms = self.task_router.select_llms(task_analysis)
            responses = self._execute_with_llms(task, selected_llms)
            final_response = self.response_aggregator.aggregate(responses, task_analysis)

            _memory.store_experience('multi_llm_router', {
                'task': task,
                'selected_llms': selected_llms,
                'final_response': final_response,
            })
            return final_response
        except Exception as e:
            logger.error(f"Multi-LLM routing failed: {e}")
            return {'error': str(e), 'fallback': True}

    def _execute_with_llms(self, task: Dict[str, Any], llms: List[str]) -> List[Dict[str, Any]]:
        responses = []
        for llm_name in llms:
            try:
                response = self._call_llm(llm_name, task)
                responses.append({'llm': llm_name, 'response': response, 'success': True,
                                   'timestamp': datetime.utcnow().isoformat()})
            except Exception as e:
                responses.append({'llm': llm_name, 'error': str(e), 'success': False,
                                   'timestamp': datetime.utcnow().isoformat()})
        return responses

    def _call_llm(self, llm_name: str, task: Dict[str, Any]) -> Dict[str, Any]:
        """Call LLM â€” uses revenue engine when wired, otherwise returns a placeholder."""
        if self.revenue_engine:
            import asyncio
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = None

            system_prompt = "You are a multi-purpose AI assistant."
            user_prompt = task.get('description', json.dumps(task))

            if loop and loop.is_running():
                # We're inside an async context â€” schedule and return a future placeholder
                future = asyncio.ensure_future(
                    self.revenue_engine.generate_ai_content(system_prompt, user_prompt, max_tokens=500)
                )
                # Synchronous callers cannot await; return what we have
                return {
                    'content': f"[async call scheduled for {llm_name}]",
                    'model_used': llm_name,
                    'tokens_used': 0,
                    'confidence': 0.5,
                }
            else:
                content = asyncio.run(
                    self.revenue_engine.generate_ai_content(system_prompt, user_prompt, max_tokens=500)
                )
                return {
                    'content': content,
                    'model_used': llm_name,
                    'tokens_used': 0,
                    'confidence': 0.8,
                }

        # No revenue engine â€” return honest placeholder
        logger.warning(f"MultiLLMRouter: no revenue engine wired, returning placeholder for {llm_name}")
        return {
            'content': f"[no LLM configured â€” placeholder for {llm_name}]",
            'model_used': llm_name,
            'tokens_used': 0,
            'confidence': 0.0,
        }


class TaskRouter:
    """Route tasks to appropriate LLMs based on task characteristics"""

    def __init__(self):
        self.llm_capabilities = {
            'code_analysis': ['anthropic', 'openai', 'xai'],
            'creative_writing': ['openai', 'openrouter'],
            'technical_explanation': ['anthropic', 'xai'],
            'data_analysis': ['openai', 'anthropic'],
            'general_conversation': ['openai', 'anthropic'],
        }

    def analyze_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        desc = task.get('description', '')
        task_type = task.get('type', 'general')

        if any(kw in desc.lower() for kw in ['code', 'programming', 'debug']):
            task_type = 'code_analysis'
        elif any(kw in desc.lower() for kw in ['write', 'create', 'generate']):
            task_type = 'creative_writing'
        elif any(kw in desc.lower() for kw in ['explain', 'understand', 'how']):
            task_type = 'technical_explanation'
        elif any(kw in desc.lower() for kw in ['analyze', 'data', 'statistics']):
            task_type = 'data_analysis'

        return {
            'task_type': task_type,
            'complexity': self._estimate_complexity(desc),
            'required_capabilities': self._extract_capabilities(desc),
        }

    def _estimate_complexity(self, text: str) -> float:
        word_count = len(text.split())
        if word_count > 200:
            return 0.9
        elif word_count > 50:
            return 0.6
        return 0.3

    def _extract_capabilities(self, text: str) -> List[str]:
        caps = []
        text_lower = text.lower()
        if any(w in text_lower for w in ['code', 'function', 'class']):
            caps.append('code_understanding')
        if any(w in text_lower for w in ['analyze', 'data']):
            caps.append('data_analysis')
        if any(w in text_lower for w in ['creative', 'write', 'story']):
            caps.append('creative_generation')
        return caps

    def select_llms(self, task_analysis: Dict[str, Any]) -> List[str]:
        task_type = task_analysis['task_type']
        complexity = task_analysis['complexity']
        candidates = self.llm_capabilities.get(task_type, ['openai'])
        if complexity > 0.8:
            return candidates[:3]
        elif complexity > 0.5:
            return candidates[:2]
        return [candidates[0]]


class ResponseAggregator:
    """Aggregate responses from multiple LLMs"""

    def aggregate(self, responses: List[Dict[str, Any]], task_analysis: Dict[str, Any]) -> Dict[str, Any]:
        successful = [r for r in responses if r.get('success')]
        if not successful:
            return {'error': 'All LLMs failed', 'responses': responses}
        if len(successful) == 1:
            return successful[0]['response']
        combined = []
        for r in successful:
            combined.append(f"From {r['llm']}: {r['response'].get('content', '')}")
        return {
            'content': '\n\n'.join(combined),
            'source_llms': [r['llm'] for r in successful],
            'aggregation_method': 'concatenation',
            'response_count': len(successful),
        }


# â”€â”€ SelfRepairEngine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class SelfRepairEngine:
    """Self-repairing system for CHATTY"""

    def __init__(self):
        self.monitor = SystemMonitor()
        self.repair_kit = RepairKit()
        self.knowledge_base = RepairKnowledgeBase()

    async def heal_system(self, error: Dict[str, Any]) -> Dict[str, Any]:
        try:
            diagnosis = await self._diagnose_error(error)
            repair_plan = self._generate_repair_plan(diagnosis)
            repair_result = self._execute_repair(repair_plan)
            validation = self._validate_fix(error, repair_result)

            return {
                'success': validation['success'],
                'diagnosis': diagnosis,
                'repair_plan': repair_plan,
                'repair_result': repair_result,
                'validation': validation,
            }
        except Exception as e:
            logger.error(f"Self-repair failed: {e}")
            return {'success': False, 'error': str(e)}

    async def _diagnose_error(self, error: Dict[str, Any]) -> Dict[str, Any]:
        error_type = error.get('type', 'unknown')
        error_message = error.get('message', '')
        return {
            'error_type': error_type,
            'severity': self._assess_severity(error_message),
            'affected_components': self._identify_affected_components(error),
            'root_cause': self.knowledge_base.get_known_cause(error_type) or 'unknown',
            'recommended_actions': self.knowledge_base.get_recommended_actions(error_type),
        }

    def _assess_severity(self, msg: str) -> str:
        msg_lower = msg.lower()
        if any(kw in msg_lower for kw in ['crash', 'fatal', 'critical', 'system failure']):
            return 'critical'
        if any(kw in msg_lower for kw in ['warning', 'timeout', 'retry', 'degraded']):
            return 'warning'
        return 'info'

    def _identify_affected_components(self, error: Dict[str, Any]) -> List[str]:
        affected = []
        mapping = {
            'database': ['database', 'sql', 'connection'],
            'api': ['api', 'endpoint', 'http'],
            'llm': ['llm', 'model', 'ai'],
            'file': ['file', 'io', 'disk'],
            'network': ['network', 'connection', 'timeout'],
        }
        text = f"{error.get('type', '')} {error.get('message', '')}".lower()
        for comp, keywords in mapping.items():
            if any(kw in text for kw in keywords):
                affected.append(comp)
        return affected

    def _generate_repair_plan(self, diagnosis: Dict[str, Any]) -> List[Dict[str, Any]]:
        actions = []
        for action in diagnosis.get('recommended_actions', []):
            actions.append({
                'action': action,
                'priority': 1 if diagnosis['severity'] == 'critical' else 0,
                'estimated_time': '30s',
            })
        actions.sort(key=lambda x: x['priority'], reverse=True)
        return actions

    def _execute_repair(self, repair_plan: List[Dict[str, Any]]) -> Dict[str, Any]:
        results = []
        for action in repair_plan:
            try:
                result = self.repair_kit.execute_action(action['action'])
                results.append({
                    'action': action['action'], 'success': result['success'],
                    'details': result.get('details', {}),
                    'timestamp': datetime.utcnow().isoformat(),
                })
                if not result['success']:
                    break
            except Exception as e:
                results.append({
                    'action': action['action'], 'success': False, 'error': str(e),
                    'timestamp': datetime.utcnow().isoformat(),
                })
                break
        return {
            'actions_executed': len(results),
            'successful_actions': sum(1 for r in results if r['success']),
            'results': results,
        }

    def _validate_fix(self, original_error: Dict[str, Any], repair_result: Dict[str, Any]) -> Dict[str, Any]:
        validation = {
            'success': repair_result['successful_actions'] > 0,
            'validation_method': 'error_resolution_check',
        }
        _memory.store_experience('self_repair', {
            'original_error': original_error,
            'repair_result': repair_result,
            'validation': validation,
        })
        return validation


class SystemMonitor:
    """Monitor system health and detect issues"""

    def __init__(self):
        self.metrics: Dict[str, Any] = {}
        self.alerts: List[Dict[str, Any]] = []

    def check_health(self) -> Dict[str, Any]:
        health = {
            'timestamp': datetime.utcnow().isoformat(),
            'components': {},
            'overall_status': 'healthy',
            'alerts': [],
        }
        for comp in ['api_server', 'llm_connections', 'file_system']:
            status = self._check_component(comp)
            health['components'][comp] = status
            if status['status'] != 'healthy':
                health['alerts'].append({
                    'component': comp, 'status': status['status'], 'message': status['message'],
                })
        unhealthy = sum(1 for c in health['components'].values() if c['status'] != 'healthy')
        if unhealthy:
            health['overall_status'] = 'degraded' if unhealthy == 1 else 'critical'
        return health

    def _check_component(self, component: str) -> Dict[str, Any]:
        return {
            'status': 'healthy',
            'message': f'{component} is operational',
            'last_check': datetime.utcnow().isoformat(),
        }


class RepairKit:
    """Collection of repair actions"""

    def __init__(self):
        self.repair_actions = {
            'restart_service': self._restart_service,
            'clear_cache': self._clear_cache,
            'reconnect_database': self._reconnect_database,
        }

    def execute_action(self, action_name: str, **kwargs) -> Dict[str, Any]:
        if action_name in self.repair_actions:
            try:
                result = self.repair_actions[action_name](**kwargs)
                return {'success': True, 'details': result}
            except Exception as e:
                return {'success': False, 'error': str(e)}
        return {'success': False, 'error': f'Unknown repair action: {action_name}'}

    def _restart_service(self, **_kw) -> Dict[str, Any]:
        return {'action': 'restart_service', 'status': 'completed'}

    def _clear_cache(self, **_kw) -> Dict[str, Any]:
        return {'action': 'clear_cache', 'cleared': ['memory_cache']}

    def _reconnect_database(self, **_kw) -> Dict[str, Any]:
        return {'action': 'reconnect_database', 'status': 'completed'}


class RepairKnowledgeBase:
    """Knowledge base for common repairs"""

    def __init__(self):
        self.known_issues = {
            'database_connection_error': {
                'root_cause': 'Database connection timeout or authentication failure',
                'recommended_actions': ['reconnect_database', 'clear_cache'],
                'severity': 'high',
            },
            'llm_api_timeout': {
                'root_cause': 'LLM API service unavailable or rate limiting',
                'recommended_actions': ['clear_cache'],
                'severity': 'medium',
            },
            'file_permission_error': {
                'root_cause': 'Insufficient file system permissions',
                'recommended_actions': [],
                'severity': 'medium',
            },
        }

    def get_recommended_actions(self, error_type: str) -> List[str]:
        issue = self.known_issues.get(error_type)
        return issue.get('recommended_actions', []) if issue else []

    def get_known_cause(self, error_type: str) -> Optional[str]:
        issue = self.known_issues.get(error_type)
        return issue.get('root_cause') if issue else None


# â”€â”€ AutonomousLearningSystem (main entry point) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class AutonomousLearningSystem:
    """Main autonomous learning and self-repairing system"""

    def __init__(self, revenue_engine=None):
        self.file_chunker = FileChunker()
        self.multi_llm_router = MultiLLMRouter(revenue_engine=revenue_engine)
        self.self_repair_engine = SelfRepairEngine()
        self.system_monitor = SystemMonitor()
        self.learning_active = False
        self.monitoring_active = False

    async def start_autonomous_system(self):
        """Start the autonomous learning and self-repairing system"""
        logger.info("ðŸš€ Starting CHATTY OpenClaw Autonomous Learning System")
        self.learning_active = True
        self.monitoring_active = True
        asyncio.create_task(self._learning_loop())
        asyncio.create_task(self._monitoring_loop())
        asyncio.create_task(self._self_repair_loop())
        logger.info("âœ… OpenClaw autonomous system started")

    async def stop_autonomous_system(self):
        self.learning_active = False
        self.monitoring_active = False
        logger.info("ðŸ›‘ OpenClaw autonomous system stopped")

    async def _learning_loop(self):
        while self.learning_active:
            try:
                await self._learn_from_files()
                await asyncio.sleep(300)
            except Exception as e:
                logger.error(f"Learning loop error: {e}")
                await asyncio.sleep(60)

    async def _learn_from_files(self):
        key_dirs = ['.']
        for directory in key_dirs:
            if not os.path.exists(directory):
                continue
            for file_path in Path(directory).glob('*.py'):
                try:
                    if self._is_recently_modified(file_path):
                        chunks = self.file_chunker.chunk_file(str(file_path))
                        knowledge = self._extract_knowledge_from_chunks(chunks)
                        _memory.store_experience('file_learning', {
                            'file_path': str(file_path),
                            'chunks': len(chunks),
                            'knowledge': knowledge,
                        })
                except Exception as e:
                    logger.warning(f"Failed to learn from {file_path}: {e}")

    def _is_recently_modified(self, file_path: Path) -> bool:
        try:
            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
            return (datetime.now() - mod_time).total_seconds() < 3600
        except Exception:
            return False

    def _extract_knowledge_from_chunks(self, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        knowledge: Dict[str, List[str]] = {
            'functions': [], 'classes': [], 'imports': [],
        }
        for chunk in chunks:
            content = chunk['content']
            knowledge['functions'].extend(re.findall(r'def\s+(\w+)', content))
            knowledge['classes'].extend(re.findall(r'class\s+(\w+)', content))
            knowledge['imports'].extend(re.findall(r'import\s+(\w+)', content))
        for key in knowledge:
            knowledge[key] = list(set(knowledge[key]))
        return knowledge

    async def _monitoring_loop(self):
        while self.monitoring_active:
            try:
                health = self.system_monitor.check_health()
                if health['overall_status'] != 'healthy':
                    logger.warning(f"System health: {health['overall_status']}")
                    for alert in health.get('alerts', []):
                        await self.self_repair_engine.heal_system(alert)
                await asyncio.sleep(60)
            except Exception as e:
                logger.error(f"Monitoring loop error: {e}")
                await asyncio.sleep(60)

    async def _self_repair_loop(self):
        while self.monitoring_active:
            try:
                experiences = _memory.get_recent_experiences(limit=50)
                for exp in experiences:
                    if exp.get('type') == 'error':
                        await self.self_repair_engine.heal_system(exp.get('content', {}))
                await asyncio.sleep(300)
            except Exception as e:
                logger.error(f"Self-repair loop error: {e}")
                await asyncio.sleep(300)


# â”€â”€ Module-level convenience â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

autonomous_system = AutonomousLearningSystem()


async def integrate_openclaw_features():
    """Integrate OpenClaw features into CHATTY"""
    logger.info("ðŸ”— Integrating OpenClaw features into CHATTY")
    await autonomous_system.start_autonomous_system()
    logger.info("âœ… OpenClaw integration complete")


if __name__ == "__main__":
    asyncio.run(integrate_openclaw_features())
