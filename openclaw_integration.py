#!/usr/bin/env python3
"""
CHATTY OpenClaw Integration
Autonomous learning and self-repairing system with multi-LLM orchestration and file chunking
"""

import os
import json
import time
import asyncio
import threading
import logging
import hashlib
import ast
import re
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Union
from pathlib import Path
from collections import defaultdict, deque
import concurrent.futures

import requests
from sentence_transformers import SentenceTransformer
import networkx as nx
from sklearn.cluster import KMeans
import numpy as np

from server import db, Agent, Task, logger
from learning_system import memory_system, adaptive_learning

class FileChunker:
    """OpenClaw-style file chunking and context management"""
    
    def __init__(self):
        self.embeddings_model = None
        try:
            self.embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')
        except:
            logger.warning("Sentence transformer not available for file chunking")
        
        self.chunk_cache = {}
        self.context_graph = nx.DiGraph()
        
    def chunk_file(self, file_path: str, max_chunk_size: int = 2000) -> List[Dict[str, Any]]:
        """Intelligently chunk files based on semantic boundaries"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            file_type = Path(file_path).suffix.lower()
            
            if file_type in ['.py', '.js', '.ts', '.java', '.cpp', '.c']:
                chunks = self._chunk_code_file(content, file_path, max_chunk_size)
            elif file_type in ['.md', '.txt', '.rst']:
                chunks = self._chunk_text_file(content, file_path, max_chunk_size)
            else:
                chunks = self._chunk_generic_file(content, file_path, max_chunk_size)
            
            # Build context relationships
            self._build_context_graph(chunks, file_path)
            
            # Cache chunks
            self.chunk_cache[file_path] = chunks
            
            logger.info(f"Chunked {file_path} into {len(chunks)} chunks")
            return chunks
            
        except Exception as e:
            logger.error(f"Failed to chunk file {file_path}: {str(e)}")
            return []
    
    def _chunk_code_file(self, content: str, file_path: str, max_size: int) -> List[Dict[str, Any]]:
        """Chunk code files by functions, classes, and logical boundaries"""
        chunks = []
        current_chunk = {
            'content': '',
            'start_line': 0,
            'end_line': 0,
            'type': 'code',
            'semantic_boundaries': []
        }
        
        lines = content.split('\n')
        current_size = 0
        chunk_id = 0
        
        for i, line in enumerate(lines):
            line_size = len(line)
            
            # Check for semantic boundaries
            boundary_type = self._detect_code_boundary(line, i, lines)
            
            if boundary_type and current_size > max_size * 0.7:
                # Create new chunk at boundary
                if current_chunk['content']:
                    chunks.append(self._finalize_chunk(current_chunk, chunk_id, file_path))
                    chunk_id += 1
                
                current_chunk = {
                    'content': line + '\n',
                    'start_line': i,
                    'end_line': i,
                    'type': 'code',
                    'semantic_boundaries': [boundary_type]
                }
                current_size = line_size
            else:
                current_chunk['content'] += line + '\n'
                current_chunk['end_line'] = i
                current_size += line_size
            
            if current_size >= max_size:
                chunks.append(self._finalize_chunk(current_chunk, chunk_id, file_path))
                chunk_id += 1
                current_chunk = {
                    'content': '',
                    'start_line': i + 1,
                    'end_line': i + 1,
                    'type': 'code',
                    'semantic_boundaries': []
                }
                current_size = 0
        
        # Add final chunk
        if current_chunk['content']:
            chunks.append(self._finalize_chunk(current_chunk, chunk_id, file_path))
        
        return chunks
    
    def _detect_code_boundary(self, line: str, line_num: int, lines: List[str]) -> Optional[str]:
        """Detect semantic boundaries in code"""
        stripped = line.strip()
        
        # Function definitions
        if re.match(r'^\s*(def|function|async\s+function)\s+\w+', stripped):
            return 'function_definition'
        
        # Class definitions
        if re.match(r'^\s*(class|interface|type)\s+\w+', stripped):
            return 'class_definition'
        
        # Import statements
        if re.match(r'^\s*(import|from|require)\s+', stripped):
            return 'import_statement'
        
        # Comments (potential section boundaries)
        if stripped.startswith('#') or stripped.startswith('//') or stripped.startswith('/*'):
            return 'comment_boundary'
        
        # Empty lines after significant code
        if not stripped and line_num > 0:
            prev_line = lines[line_num - 1].strip()
            if prev_line and not prev_line.startswith('#') and not prev_line.startswith('//'):
                return 'logical_boundary'
        
        return None
    
    def _chunk_text_file(self, content: str, file_path: str, max_size: int) -> List[Dict[str, Any]]:
        """Chunk text files by paragraphs and headings"""
        chunks = []
        paragraphs = content.split('\n\n')
        
        current_chunk = {
            'content': '',
            'start_paragraph': 0,
            'end_paragraph': 0,
            'type': 'text',
            'semantic_boundaries': []
        }
        
        current_size = 0
        chunk_id = 0
        
        for i, paragraph in enumerate(paragraphs):
            para_size = len(paragraph)
            
            # Check for heading boundaries
            if paragraph.startswith('#') or paragraph.startswith('##'):
                if current_size > max_size * 0.8 and current_chunk['content']:
                    chunks.append(self._finalize_chunk(current_chunk, chunk_id, file_path))
                    chunk_id += 1
                    current_chunk = {
                        'content': paragraph + '\n\n',
                        'start_paragraph': i,
                        'end_paragraph': i,
                        'type': 'text',
                        'semantic_boundaries': ['heading']
                    }
                    current_size = para_size
                    continue
            
            if current_size + para_size > max_size:
                if current_chunk['content']:
                    chunks.append(self._finalize_chunk(current_chunk, chunk_id, file_path))
                    chunk_id += 1
                
                current_chunk = {
                    'content': paragraph + '\n\n',
                    'start_paragraph': i,
                    'end_paragraph': i,
                    'type': 'text',
                    'semantic_boundaries': []
                }
                current_size = para_size
            else:
                current_chunk['content'] += paragraph + '\n\n'
                current_chunk['end_paragraph'] = i
                current_size += para_size
        
        if current_chunk['content']:
            chunks.append(self._finalize_chunk(current_chunk, chunk_id, file_path))
        
        return chunks
    
    def _chunk_generic_file(self, content: str, file_path: str, max_size: int) -> List[Dict[str, Any]]:
        """Generic chunking for other file types"""
        chunks = []
        lines = content.split('\n')
        
        for i in range(0, len(lines), max_size // 100):  # Rough line-based chunking
            chunk_lines = lines[i:i + max_size // 100]
            chunk_content = '\n'.join(chunk_lines)
            
            chunks.append({
                'content': chunk_content,
                'start_line': i,
                'end_line': min(i + max_size // 100 - 1, len(lines) - 1),
                'type': 'generic',
                'semantic_boundaries': []
            })
        
        return chunks
    
    def _finalize_chunk(self, chunk_data: Dict[str, Any], chunk_id: int, file_path: str) -> Dict[str, Any]:
        """Finalize chunk with metadata and embeddings"""
        chunk_id_str = f"{file_path}#{chunk_id}"
        
        # Generate semantic embedding if model available
        embedding = None
        if self.embeddings_model:
            try:
                embedding = self.embeddings_model.encode(chunk_data['content']).tolist()
            except Exception as e:
                logger.warning(f"Failed to generate embedding for chunk {chunk_id_str}: {str(e)}")
        
        return {
            'id': chunk_id_str,
            'file_path': file_path,
            'content': chunk_data['content'],
            'start_line': chunk_data['start_line'],
            'end_line': chunk_data['end_line'],
            'type': chunk_data['type'],
            'semantic_boundaries': chunk_data['semantic_boundaries'],
            'embedding': embedding,
            'size': len(chunk_data['content']),
            'created_at': datetime.utcnow().isoformat()
        }
    
    def _build_context_graph(self, chunks: List[Dict[str, Any]], file_path: str):
        """Build semantic context graph between chunks"""
        for i, chunk in enumerate(chunks):
            chunk_id = chunk['id']
            
            # Add node
            self.context_graph.add_node(chunk_id, **{
                'file_path': file_path,
                'type': chunk['type'],
                'size': chunk['size'],
                'boundaries': chunk['semantic_boundaries']
            })
            
            # Add edges to adjacent chunks
            if i > 0:
                prev_chunk = chunks[i - 1]
                self.context_graph.add_edge(prev_chunk['id'], chunk_id, relationship='sequential')
            
            # Add semantic similarity edges (if embeddings available)
            if chunk.get('embedding'):
                for j, other_chunk in enumerate(chunks):
                    if i != j and other_chunk.get('embedding'):
                        similarity = self._calculate_similarity(
                            chunk['embedding'], other_chunk['embedding']
                        )
                        if similarity > 0.7:  # High similarity threshold
                            self.context_graph.add_edge(
                                chunk_id, other_chunk['id'],
                                relationship='semantic_similarity',
                                weight=similarity
                            )
    
    def _calculate_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """Calculate cosine similarity between two vectors"""
        try:
            import numpy as np
            v1 = np.array(vec1)
            v2 = np.array(vec2)
            return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
        except:
            return 0.0
    
    def get_relevant_chunks(self, query: str, file_path: str = None, top_k: int = 5) -> List[Dict[str, Any]]:
        """Get chunks most relevant to a query"""
        if not self.embeddings_model:
            return []
        
        try:
            query_embedding = self.embeddings_model.encode(query)
            relevant_chunks = []
            
            # Search in specific file or all files
            target_chunks = []
            if file_path and file_path in self.chunk_cache:
                target_chunks = self.chunk_cache[file_path]
            elif file_path is None:
                for chunks in self.chunk_cache.values():
                    target_chunks.extend(chunks)
            else:
                return []
            
            # Calculate similarities
            for chunk in target_chunks:
                if chunk.get('embedding'):
                    similarity = self._calculate_similarity(query_embedding, chunk['embedding'])
                    relevant_chunks.append((chunk, similarity))
            
            # Sort by similarity and return top k
            relevant_chunks.sort(key=lambda x: x[1], reverse=True)
            return [chunk for chunk, sim in relevant_chunks[:top_k]]
            
        except Exception as e:
            logger.error(f"Failed to get relevant chunks: {str(e)}")
            return []

class MultiLLMRouter:
    """OpenClaw-style multi-LLM orchestration"""
    
    def __init__(self):
        self.llm_pool = {}
        self.task_router = TaskRouter()
        self.response_aggregator = ResponseAggregator()
        
    def route_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """Route task to optimal LLM combination"""
        try:
            # Analyze task requirements
            task_analysis = self.task_router.analyze_task(task)
            
            # Select LLMs based on task type and requirements
            selected_llms = self.task_router.select_llms(task_analysis)
            
            # Execute task with selected LLMs
            responses = self._execute_with_llms(task, selected_llms)
            
            # Aggregate responses
            final_response = self.response_aggregator.aggregate(responses, task_analysis)
            
            # Store in learning system
            memory_system.store_experience('multi_llm_router', {
                'task': task,
                'selected_llms': selected_llms,
                'responses': responses,
                'final_response': final_response
            })
            
            return final_response
            
        except Exception as e:
            logger.error(f"Multi-LLM routing failed: {str(e)}")
            return {'error': str(e), 'fallback': True}
    
    def _execute_with_llms(self, task: Dict[str, Any], llms: List[str]) -> List[Dict[str, Any]]:
        """Execute task with multiple LLMs"""
        responses = []
        
        for llm_name in llms:
            try:
                response = self._call_llm(llm_name, task)
                responses.append({
                    'llm': llm_name,
                    'response': response,
                    'success': True,
                    'timestamp': datetime.utcnow().isoformat()
                })
            except Exception as e:
                responses.append({
                    'llm': llm_name,
                    'error': str(e),
                    'success': False,
                    'timestamp': datetime.utcnow().isoformat()
                })
        
        return responses
    
    def _call_llm(self, llm_name: str, task: Dict[str, Any]) -> Dict[str, Any]:
        """Call specific LLM with task"""
        # This would integrate with your existing LLM infrastructure
        # For now, return a mock response
        return {
            'content': f"Response from {llm_name} for task: {task.get('description', 'unknown')}",
            'model_used': llm_name,
            'tokens_used': 100,
            'confidence': 0.8
        }

class TaskRouter:
    """Route tasks to appropriate LLMs based on task characteristics"""
    
    def __init__(self):
        self.llm_capabilities = {
            'code_analysis': ['anthropic', 'openai', 'xai'],
            'creative_writing': ['openai', 'cohere', 'openrouter'],
            'technical_explanation': ['anthropic', 'xai', 'google'],
            'data_analysis': ['openai', 'google', 'mistral'],
            'general_conversation': ['openai', 'anthropic', 'cohere']
        }
    
    def analyze_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze task to determine optimal LLM"""
        task_description = task.get('description', '')
        task_type = task.get('type', 'general')
        
        # Simple task type detection
        if any(keyword in task_description.lower() for keyword in ['code', 'programming', 'debug']):
            task_type = 'code_analysis'
        elif any(keyword in task_description.lower() for keyword in ['write', 'create', 'generate']):
            task_type = 'creative_writing'
        elif any(keyword in task_description.lower() for keyword in ['explain', 'understand', 'how']):
            task_type = 'technical_explanation'
        elif any(keyword in task_description.lower() for keyword in ['analyze', 'data', 'statistics']):
            task_type = 'data_analysis'
        
        return {
            'task_type': task_type,
            'complexity': self._estimate_complexity(task_description),
            'required_capabilities': self._extract_capabilities(task_description)
        }
    
    def select_llms(self, task_analysis: Dict[str, Any]) -> List[str]:
        """Select optimal LLMs for the task"""
        task_type = task_analysis['task_type']
        complexity = task_analysis['complexity']
        
        # Get candidate LLMs for task type
        candidates = self.llm_capabilities.get(task_type, ['openai'])
        
        # Filter based on complexity (simplified)
        if complexity > 0.8:  # High complexity
            # Use multiple LLMs for consensus
            return candidates[:3]
        elif complexity > 0.5:  # Medium complexity
            return candidates[:2]
        else:  # Low complexity
            return [candidates[0]]

class ResponseAggregator:
    """Aggregate responses from multiple LLMs"""
    
    def aggregate(self, responses: List[Dict[str, Any]], task_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Aggregate responses using voting/consensus"""
        successful_responses = [r for r in responses if r.get('success')]
        
        if not successful_responses:
            return {'error': 'All LLMs failed', 'responses': responses}
        
        # Simple majority voting for text responses
        if len(successful_responses) == 1:
            return successful_responses[0]['response']
        
        # For multiple responses, combine them
        combined_content = []
        for response in successful_responses:
            content = response['response'].get('content', '')
            combined_content.append(f"From {response['llm']}: {content}")
        
        return {
            'content': '\n\n'.join(combined_content),
            'source_llms': [r['llm'] for r in successful_responses],
            'aggregation_method': 'concatenation',
            'response_count': len(successful_responses)
        }

class SelfRepairEngine:
    """Self-repairing system for CHATTY"""
    
    def __init__(self):
        self.monitor = SystemMonitor()
        self.repair_kit = RepairKit()
        self.knowledge_base = RepairKnowledgeBase()
        
    async def heal_system(self, error: Dict[str, Any]) -> Dict[str, Any]:
        """Heal system errors automatically"""
        try:
            # Diagnose the error
            diagnosis = await self._diagnose_error(error)
            
            # Generate repair plan
            repair_plan = self._generate_repair_plan(diagnosis)
            
            # Execute repair
            repair_result = await self._execute_repair(repair_plan)
            
            # Validate fix
            validation_result = await self._validate_fix(error, repair_result)
            
            return {
                'success': validation_result['success'],
                'diagnosis': diagnosis,
                'repair_plan': repair_plan,
                'repair_result': repair_result,
                'validation': validation_result
            }
            
        except Exception as e:
            logger.error(f"Self-repair failed: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    async def _diagnose_error(self, error: Dict[str, Any]) -> Dict[str, Any]:
        """Diagnose system error"""
        error_type = error.get('type', 'unknown')
        error_message = error.get('message', '')
        
        # Analyze error patterns
        diagnosis = {
            'error_type': error_type,
            'severity': self._assess_severity(error_message),
            'affected_components': self._identify_affected_components(error),
            'root_cause': await self._identify_root_cause(error),
            'recommended_actions': self.knowledge_base.get_recommended_actions(error_type)
        }
        
        return diagnosis
    
    def _assess_severity(self, error_message: str) -> str:
        """Assess error severity"""
        critical_keywords = ['crash', 'fatal', 'critical', 'system failure']
        warning_keywords = ['warning', 'timeout', 'retry', 'degraded']
        
        error_lower = error_message.lower()
        
        if any(keyword in error_lower for keyword in critical_keywords):
            return 'critical'
        elif any(keyword in error_lower for keyword in warning_keywords):
            return 'warning'
        else:
            return 'info'
    
    def _identify_affected_components(self, error: Dict[str, Any]) -> List[str]:
        """Identify which components are affected"""
        affected = []
        
        # Map error types to components
        component_mapping = {
            'database': ['database', 'sql', 'connection'],
            'api': ['api', 'endpoint', 'http'],
            'llm': ['llm', 'model', 'ai'],
            'file': ['file', 'io', 'disk'],
            'network': ['network', 'connection', 'timeout']
        }
        
        error_text = f"{error.get('type', '')} {error.get('message', '')}".lower()
        
        for component, keywords in component_mapping.items():
            if any(keyword in error_text for keyword in keywords):
                affected.append(component)
        
        return affected
    
    async def _identify_root_cause(self, error: Dict[str, Any]) -> str:
        """Identify root cause of error"""
        # Use AI to analyze error patterns
        error_analysis = await self._analyze_error_pattern(error)
        
        # Check known issues
        known_cause = self.knowledge_base.get_known_cause(error.get('type'))
        if known_cause:
            return known_cause
        
        return error_analysis.get('root_cause', 'unknown')
    
    def _generate_repair_plan(self, diagnosis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate step-by-step repair plan"""
        repair_actions = []
        
        # Add recommended actions from knowledge base
        for action in diagnosis.get('recommended_actions', []):
            repair_actions.append({
                'action': action,
                'priority': self._calculate_priority(action, diagnosis['severity']),
                'estimated_time': self._estimate_repair_time(action)
            })
        
        # Sort by priority
        repair_actions.sort(key=lambda x: x['priority'], reverse=True)
        
        return repair_actions
    
    def _execute_repair(self, repair_plan: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Execute repair plan"""
        results = []
        
        for action in repair_plan:
            try:
                result = self.repair_kit.execute_action(action['action'])
                results.append({
                    'action': action['action'],
                    'success': result['success'],
                    'details': result.get('details', {}),
                    'timestamp': datetime.utcnow().isoformat()
                })
                
                if not result['success']:
                    break  # Stop on first failure
                    
            except Exception as e:
                results.append({
                    'action': action['action'],
                    'success': False,
                    'error': str(e),
                    'timestamp': datetime.utcnow().isoformat()
                })
                break
        
        return {
            'actions_executed': len(results),
            'successful_actions': sum(1 for r in results if r['success']),
            'results': results
        }
    
    async def _validate_fix(self, original_error: Dict[str, Any], repair_result: Dict[str, Any]) -> Dict[str, Any]:
        """Validate that the fix resolved the issue"""
        # Simple validation - check if error reoccurs
        validation = {
            'success': repair_result['successful_actions'] > 0,
            'validation_method': 'error_resolution_check',
            'validation_details': {
                'original_error': original_error,
                'repair_outcome': repair_result
            }
        }
        
        # Store repair experience
        memory_system.store_experience('self_repair', {
            'original_error': original_error,
            'repair_plan': repair_result,
            'validation': validation,
            'outcome': 'success' if validation['success'] else 'failure'
        })
        
        return validation

class SystemMonitor:
    """Monitor system health and detect issues"""
    
    def __init__(self):
        self.metrics = {}
        self.alerts = []
        
    def check_health(self) -> Dict[str, Any]:
        """Check overall system health"""
        health_status = {
            'timestamp': datetime.utcnow().isoformat(),
            'components': {},
            'overall_status': 'healthy',
            'alerts': []
        }
        
        # Check key components
        components = ['database', 'api_server', 'llm_connections', 'file_system']
        
        for component in components:
            status = self._check_component(component)
            health_status['components'][component] = status
            
            if status['status'] != 'healthy':
                health_status['alerts'].append({
                    'component': component,
                    'status': status['status'],
                    'message': status['message']
                })
        
        # Determine overall status
        unhealthy_count = sum(1 for comp in health_status['components'].values() if comp['status'] != 'healthy')
        if unhealthy_count > 0:
            health_status['overall_status'] = 'degraded' if unhealthy_count == 1 else 'critical'
        
        return health_status
    
    def _check_component(self, component: str) -> Dict[str, Any]:
        """Check individual component health"""
        # This would integrate with your existing monitoring
        # For now, return mock status
        return {
            'status': 'healthy',
            'message': f'{component} is operational',
            'last_check': datetime.utcnow().isoformat(),
            'metrics': {}
        }

class RepairKit:
    """Collection of repair actions"""
    
    def __init__(self):
        self.repair_actions = {
            'restart_service': self._restart_service,
            'clear_cache': self._clear_cache,
            'reconnect_database': self._reconnect_database,
            'update_configuration': self._update_configuration,
            'install_dependency': self._install_dependency
        }
    
    def execute_action(self, action_name: str, **kwargs) -> Dict[str, Any]:
        """Execute specific repair action"""
        if action_name in self.repair_actions:
            try:
                result = self.repair_actions[action_name](**kwargs)
                return {'success': True, 'details': result}
            except Exception as e:
                return {'success': False, 'error': str(e)}
        else:
            return {'success': False, 'error': f'Unknown repair action: {action_name}'}
    
    def _restart_service(self, service_name: str) -> Dict[str, Any]:
        """Restart a service"""
        # Implementation would depend on your service management
        return {'action': 'restart_service', 'service': service_name, 'status': 'completed'}
    
    def _clear_cache(self) -> Dict[str, Any]:
        """Clear system cache"""
        # Clear various caches
        return {'action': 'clear_cache', 'cleared': ['memory_cache', 'file_cache']}
    
    def _reconnect_database(self) -> Dict[str, Any]:
        """Reconnect to database"""
        # Implementation would handle database reconnection
        return {'action': 'reconnect_database', 'status': 'completed'}
    
    def _update_configuration(self, config_updates: Dict[str, Any]) -> Dict[str, Any]:
        """Update system configuration"""
        return {'action': 'update_configuration', 'updates': config_updates, 'status': 'applied'}
    
    def _install_dependency(self, dependency_name: str) -> Dict[str, Any]:
        """Install missing dependency"""
        return {'action': 'install_dependency', 'dependency': dependency_name, 'status': 'installed'}

class RepairKnowledgeBase:
    """Knowledge base for common repairs"""
    
    def __init__(self):
        self.known_issues = {
            'database_connection_error': {
                'root_cause': 'Database connection timeout or authentication failure',
                'recommended_actions': ['reconnect_database', 'check_credentials', 'restart_database_service'],
                'severity': 'high'
            },
            'llm_api_timeout': {
                'root_cause': 'LLM API service unavailable or rate limiting',
                'recommended_actions': ['clear_cache', 'switch_llm_provider', 'retry_with_backoff'],
                'severity': 'medium'
            },
            'file_permission_error': {
                'root_cause': 'Insufficient file system permissions',
                'recommended_actions': ['update_file_permissions', 'check_user_permissions'],
                'severity': 'medium'
            }
        }
    
    def get_recommended_actions(self, error_type: str) -> List[str]:
        """Get recommended actions for error type"""
        issue = self.known_issues.get(error_type)
        return issue.get('recommended_actions', []) if issue else []
    
    def get_known_cause(self, error_type: str) -> Optional[str]:
        """Get known root cause for error type"""
        issue = self.known_issues.get(error_type)
        return issue.get('root_cause') if issue else None

class AutonomousLearningSystem:
    """Main autonomous learning and self-repairing system"""
    
    def __init__(self):
        self.file_chunker = FileChunker()
        self.multi_llm_router = MultiLLMRouter()
        self.self_repair_engine = SelfRepairEngine()
        self.system_monitor = SystemMonitor()
        
        # Learning loops
        self.learning_active = False
        self.monitoring_active = False
        
    async def start_autonomous_system(self):
        """Start the autonomous learning and self-repairing system"""
        logger.info("ğŸš€ Starting CHATTY Autonomous Learning System")
        
        self.learning_active = True
        self.monitoring_active = True
        
        # Start background loops
        asyncio.create_task(self._learning_loop())
        asyncio.create_task(self._monitoring_loop())
        asyncio.create_task(self._self_repair_loop())
        
        logger.info("âœ… Autonomous system started")
    
    async def stop_autonomous_system(self):
        """Stop the autonomous system"""
        self.learning_active = False
        self.monitoring_active = False
        logger.info("ğŸ›‘ Autonomous system stopped")
    
    async def _learning_loop(self):
        """Continuous learning loop"""
        while self.learning_active:
            try:
                # Learn from file changes
                await self._learn_from_files()
                
                # Learn from user interactions
                await self._learn_from_interactions()
                
                # Learn from system performance
                await self._learn_from_performance()
                
                # Optimize based on learning
                await self._optimize_system()
                
                await asyncio.sleep(300)  # Check every 5 minutes
                
            except Exception as e:
                logger.error(f"Learning loop error: {str(e)}")
                await asyncio.sleep(60)
    
    async def _learn_from_files(self):
        """Learn from file changes and content"""
        # Monitor key directories for changes
        key_dirs = ['.', 'backend/', 'api/']
        
        for directory in key_dirs:
            if os.path.exists(directory):
                for file_path in Path(directory).rglob('*.py'):
                    try:
                        # Check if file was modified recently
                        if self._is_recently_modified(file_path):
                            # Chunk and analyze file
                            chunks = self.file_chunker.chunk_file(str(file_path))
                            
                            # Extract knowledge
                            knowledge = self._extract_knowledge_from_chunks(chunks)
                            
                            # Store in learning system
                            memory_system.store_experience('file_learning', {
                                'file_path': str(file_path),
                                'chunks': len(chunks),
                                'knowledge': knowledge,
                                'learning_type': 'file_analysis'
                            })
                            
                    except Exception as e:
                        logger.warning(f"Failed to learn from file {file_path}: {str(e)}")
    
    def _is_recently_modified(self, file_path: Path) -> bool:
        """Check if file was modified in the last hour"""
        try:
            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
            return (datetime.now() - mod_time).total_seconds() < 3600
        except:
            return False
    
    def _extract_knowledge_from_chunks(self, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Extract knowledge from file chunks"""
        knowledge = {
            'patterns': [],
            'functions': [],
            'classes': [],
            'imports': [],
            'comments': []
        }
        
        for chunk in chunks:
            content = chunk['content']
            
            # Extract functions
            functions = re.findall(r'def\s+(\w+)', content)
            knowledge['functions'].extend(functions)
            
            # Extract classes
            classes = re.findall(r'class\s+(\w+)', content)
            knowledge['classes'].extend(classes)
            
            # Extract imports
            imports = re.findall(r'import\s+(\w+)', content)
            knowledge['imports'].extend(imports)
            
            # Extract comments
            comments = re.findall(r'#\s*(.+)', content)
            knowledge['comments'].extend(comments)
        
        # Remove duplicates
        for key in knowledge:
            knowledge[key] = list(set(knowledge[key]))
        
        return knowledge
    
    async def _learn_from_interactions(self):
        """Learn from user interactions and system usage"""
        # This would integrate with your existing interaction logging
        # For now, analyze recent memory entries
        recent_experiences = memory_system.get_recent_experiences(limit=10)
        
        for experience in recent_experiences:
            if experience.get('type') == 'user_interaction':
                # Extract patterns from interactions
                patterns = self._extract_interaction_patterns(experience)
                
                # Store learning
                memory_system.store_experience('interaction_learning', {
                    'patterns': patterns,
                    'source': experience.get('source'),
                    'timestamp': datetime.utcnow().isoformat()
                })
    
    def _extract_interaction_patterns(self, experience: Dict[str, Any]) -> List[str]:
        """Extract patterns from user interactions"""
        patterns = []
        
        # Analyze interaction content
        content = experience.get('content', {})
        
        if 'command' in content:
            command = content['command'].lower()
            if 'help' in command:
                patterns.append('user_needs_assistance')
            elif 'debug' in command:
                patterns.append('user_debugging')
            elif 'optimize' in command:
                patterns.append('user_optimization_request')
        
        return patterns
    
    async def _learn_from_performance(self):
        """Learn from system performance metrics"""
        # Get performance metrics
        metrics = self.system_monitor.check_health()
        
        # Analyze performance patterns
        performance_patterns = self._analyze_performance_patterns(metrics)
        
        # Store performance learning
        memory_system.store_experience('performance_learning', {
            'metrics': metrics,
            'patterns': performance_patterns,
            'timestamp': datetime.utcnow().isoformat()
        })
    
    def _analyze_performance_patterns(self, metrics: Dict[str, Any]) -> List[str]:
        """Analyze performance patterns"""
        patterns = []
        
        overall_status = metrics.get('overall_status', 'unknown')
        if overall_status != 'healthy':
            patterns.append(f'system_{overall_status}')
        
        for component, status in metrics.get('components', {}).items():
            if status.get('status') != 'healthy':
                patterns.append(f'{component}_degraded')
        
        return patterns
    
    async def _optimize_system(self):
        """Optimize system based on learning"""
        # Get recent learning experiences
        learning_experiences = memory_system.get_recent_experiences(
            experience_type='learning',
            limit=20
        )
        
        if not learning_experiences:
            return
        
        # Analyze learning for optimization opportunities
        optimization_opportunities = self._identify_optimizations(learning_experiences)
        
        # Apply optimizations
        for opportunity in optimization_opportunities:
            try:
                await self._apply_optimization(opportunity)
            except Exception as e:
                logger.error(f"Failed to apply optimization {opportunity}: {str(e)}")
    
    def _identify_optimizations(self, learning_experiences: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Identify optimization opportunities from learning"""
        optimizations = []
        
        # Look for performance issues
        for experience in learning_experiences:
            if experience.get('type') == 'performance_learning':
                metrics = experience.get('content', {}).get('metrics', {})
                if metrics.get('overall_status') != 'healthy':
                    optimizations.append({
                        'type': 'performance_optimization',
                        'priority': 'high',
                        'details': metrics
                    })
        
        # Look for code patterns that could be optimized
        for experience in learning_experiences:
            if experience.get('type') == 'file_learning':
                knowledge = experience.get('content', {}).get('knowledge', {})
                if len(knowledge.get('functions', [])) > 50:  # Large file
                    optimizations.append({
                        'type': 'code_refactoring',
                        'priority': 'medium',
                        'details': {'file': experience.get('content', {}).get('file_path')}
                    })
        
        return optimizations
    
    async def _apply_optimization(self, optimization: Dict[str, Any]):
        """Apply a specific optimization"""
        opt_type = optimization['type']
        
        if opt_type == 'performance_optimization':
            # Apply performance optimizations
            await self._optimize_performance(optimization['details'])
        elif opt_type == 'code_refactoring':
            # Suggest code refactoring
            await self._suggest_refactoring(optimization['details'])
    
    async def _optimize_performance(self, metrics: Dict[str, Any]):
        """Optimize system performance"""
        # This would implement actual performance optimizations
        logger.info("ğŸ”§ Applying performance optimizations")
    
    async def _suggest_refactoring(self, details: Dict[str, Any]):
        """Suggest code refactoring"""
        file_path = details.get('file_path')
        if file_path:
            logger.info(f"ğŸ“ Suggesting refactoring for {file_path}")
            # Generate refactoring suggestions using AI
    
    async def _monitoring_loop(self):
        """Continuous system monitoring loop"""
        while self.monitoring_active:
            try:
                # Check system health
                health_status = self.system_monitor.check_health()
                
                # Log health status
                logger.info(f"System health: {health_status['overall_status']}")
                
                # Trigger self-repair if needed
                if health_status['overall_status'] != 'healthy':
                    for alert in health_status.get('alerts', []):
                        await self.self_repair_engine.heal_system(alert)
                
                await asyncio.sleep(60)  # Check every minute
                
            except Exception as e:
                logger.error(f"Monitoring loop error: {str(e)}")
                await asyncio.sleep(60)
    
    async def _self_repair_loop(self):
        """Continuous self-repair monitoring loop"""
        while self.monitoring_active:
            try:
                # Check for repair opportunities
                repair_opportunities = await self._identify_repair_opportunities()
                
                # Execute repairs
                for opportunity in repair_opportunities:
                    await self.self_repair_engine.heal_system(opportunity)
                
                await asyncio.sleep(300)  # Check every 5 minutes
                
            except Exception as e:
                logger.error(f"Self-repair loop error: {str(e)}")
                await asyncio.sleep(300)
    
    async def _identify_repair_opportunities(self) -> List[Dict[str, Any]]:
        """Identify opportunities for self-repair"""
        opportunities = []
        
        # Check for known error patterns
        recent_experiences = memory_system.get_recent_experiences(limit=50)
        
        for experience in recent_experiences:
            if experience.get('type') == 'error':
                error_data = experience.get('content', {})
                opportunities.append({
                    'type': error_data.get('error_type', 'unknown'),
                    'message': error_data.get('message', ''),
                    'timestamp': experience.get('timestamp')
                })
        
        return opportunities

# Global instance
autonomous_system = AutonomousLearningSystem()

# Integration with existing CHATTY system
async def integrate_openclaw_features():
    """Integrate OpenClaw features into CHATTY"""
    logger.info("ğŸ”— Integrating OpenClaw features into CHATTY")
    
    # Start autonomous system
    await autonomous_system.start_autonomous_system()
    
    # Enhance existing agents with file chunking
    logger.info("ğŸ“ Enhancing agents with file chunking capabilities")
    
    # Integrate multi-LLM routing
    logger.info("ğŸ¤– Integrating multi-LLM orchestration")
    
    # Enable self-repairing capabilities
    logger.info("ğŸ”§ Enabling self-repairing system capabilities")
    
    logger.info("âœ… OpenClaw integration complete")

if __name__ == "__main__":
    # Test the integration
    asyncio.run(integrate_openclaw_features())