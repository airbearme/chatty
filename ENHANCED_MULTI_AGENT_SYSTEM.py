#!/usr/bin/env python3
"""
Enhanced Multi-Agent System with Self-Healing, YouTube Learning, and Token-Efficient Communication
Integrates OpenCLAW, Agent Zero, Pydantic AI, and other open-source tools
"""

import asyncio
import json
import logging
import os
import uuid
import time
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass, field
from enum import Enum
import concurrent.futures
from collections import defaultdict, deque

# Core AI and ML libraries
import numpy as np
from pydantic import BaseModel, Field, validator
import networkx as nx
from sentence_transformers import SentenceTransformer

# Async and communication
import aiohttp
import websockets
from asyncio import Queue, Event, gather

# YouTube and transcription
try:
    from youtube_transcript_api import YouTubeTranscriptApi
    from youtube_transcript_api.formatters import TextFormatter
    YOUTUBE_AVAILABLE = True
except ImportError:
    YOUTUBE_AVAILABLE = False

# Open-source integrations
try:
    import openclaw
    OPENCLAW_AVAILABLE = True
except ImportError:
    OPENCLAW_AVAILABLE = False

# Agent Zero integration
try:
    import agent_zero
    AGENT_ZERO_AVAILABLE = True
except ImportError:
    AGENT_ZERO_AVAILABLE = False

# N8N workflow integration
try:
    import n8n
    N8N_AVAILABLE = True
except ImportError:
    N8N_AVAILABLE = False

# Memory and vector storage
try:
    import chromadb
    CHROMA_AVAILABLE = True
except ImportError:
    CHROMA_AVAILABLE = False

# Import context management
from CONTEXT_WINDOW_MANAGER import ContextWindowManager, context_aware, global_context_manager

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/enhanced_agents.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================================================================
# CORE DATA MODELS
# ============================================================================

class AgentRole(Enum):
    """Specialized agent roles"""
    SYSTEM_OPTIMIZER = "system_optimizer"
    DATA_ANALYST = "data_analyst"
    STRATEGY_PLANNER = "strategy_planner"
    CONTENT_CREATOR = "content_creator"
    SEO_SPECIALIST = "seo_specialist"
    CUSTOMER_SUCCESS = "customer_success"
    SUPPORT_SPECIALIST = "support_specialist"
    DEVELOPER = "developer"
    DEVOPS = "devops"
    YOUTUBE_LEARNER = "youtube_learner"
    SELF_HEALER = "self_healer"
    GUARDRAIL = "guardrail"
    TOKEN_OPTIMIZER = "token_optimizer"

class MessageType(Enum):
    """Message types for agent communication"""
    REQUEST = "request"
    RESPONSE = "response"
    UPDATE = "update"
    ALERT = "alert"
    LEARNING = "learning"
    HEALING = "healing"
    GUARDRAIL_CHECK = "guardrail_check"

class Priority(Enum):
    """Message priority levels"""
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4
    EMERGENCY = 5

@dataclass
class TokenContext:
    """Token usage context for optimization"""
    input_tokens: int = 0
    output_tokens: int = 0
    context_compression_ratio: float = 1.0
    cost_estimate: float = 0.0
    optimization_applied: bool = False

@dataclass
class AgentMessage:
    """Enhanced agent message with token optimization"""
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    from_agent: str = ""
    to_agent: str = ""
    message_type: MessageType = MessageType.REQUEST
    content: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)
    priority: Priority = Priority.MEDIUM
    token_context: TokenContext = field(default_factory=TokenContext)
    compressed_content: Optional[str] = None
    requires_response: bool = True
    retry_count: int = 0
    max_retries: int = 3

@dataclass
class LearningInsight:
    """Learning insight from YouTube/videos"""
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    source: str = ""
    insight_type: str = ""
    content: str = ""
    confidence: float = 0.0
    relevance_score: float = 0.0
    keywords: List[str] = field(default_factory=list)
    timestamp: datetime = field(default_factory=datetime.now)
    integrated: bool = False

@dataclass
class HealingAction:
    """Self-healing action"""
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    issue_type: str = ""
    severity: Priority = Priority.MEDIUM
    description: str = ""
    auto_fixable: bool = True
    fix_script: Optional[str] = None
    requires_human_approval: bool = False
    timestamp: datetime = field(default_factory=datetime.now)
    status: str = "pending"

class PydanticAgentConfig(BaseModel):
    """Pydantic-based agent configuration"""
    agent_id: str
    role: AgentRole
    capabilities: List[str]
    llm_provider: str = "openai"
    model_name: str = "gpt-3.5-turbo"
    max_tokens: int = 4000
    temperature: float = 0.7
    memory_enabled: bool = True
    learning_enabled: bool = True
    auto_healing: bool = True
    token_optimization: bool = True

# ============================================================================
# TOKEN EFFICIENT COMMUNICATION SYSTEM
# ============================================================================

class TokenOptimizer:
    """Optimizes token usage across agent communications"""
    
    def __init__(self):
        self.compression_model = None
        self.token_costs = {
            "openai": {"input": 0.0015, "output": 0.002},  # per 1K tokens
            "anthropic": {"input": 0.0008, "output": 0.0024},
            "openrouter": {"input": 0.001, "output": 0.002}
        }
        self.context_cache = {}
        self._init_compression()
    
    def _init_compression(self):
        """Initialize text compression model"""
        try:
            self.compression_model = SentenceTransformer('all-MiniLM-L6-v2')
            logger.info("âœ… Token compression model loaded")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load compression model: {e}")
    
    def estimate_tokens(self, text: str, model: str = "openai") -> int:
        """Estimate token count for text"""
        # Rough estimation: ~4 characters per token
        return len(text) // 4
    
    def compress_content(self, content: Dict[str, Any]) -> Tuple[str, TokenContext]:
        """Compress content to reduce token usage"""
        try:
            content_str = json.dumps(content, separators=(',', ':'))
            original_tokens = self.estimate_tokens(content_str)
            
            # Use semantic compression for large content
            if original_tokens > 1000 and self.compression_model:
                # Extract key semantic information
                compressed = self._semantic_compress(content_str)
                compressed_tokens = self.estimate_tokens(compressed)
                compression_ratio = original_tokens / max(compressed_tokens, 1)
            else:
                compressed = content_str
                compression_ratio = 1.0
            
            token_context = TokenContext(
                input_tokens=original_tokens,
                output_tokens=compressed_tokens,
                context_compression_ratio=compression_ratio,
                cost_estimate=self._calculate_cost(original_tokens, "openai"),
                optimization_applied=compression_ratio > 1.1
            )
            
            return compressed, token_context
            
        except Exception as e:
            logger.error(f"Content compression failed: {e}")
            return json.dumps(content), TokenContext()
    
    def _semantic_compress(self, text: str) -> str:
        """Semantic compression using embeddings"""
        try:
            # Split into chunks and extract key sentences
            sentences = re.split(r'[.!?]+', text)
            if len(sentences) <= 5:
                return text
            
            # Generate embeddings for sentences
            embeddings = self.compression_model.encode(sentences)
            
            # Select diverse sentences using clustering
            selected_indices = self._select_diverse_sentences(embeddings, min(5, len(sentences)))
            selected_sentences = [sentences[i] for i in selected_indices]
            
            return '. '.join(selected_sentences)
            
        except Exception as e:
            logger.error(f"Semantic compression failed: {e}")
            return text[:1000] + "..." if len(text) > 1000 else text
    
    def _select_diverse_sentences(self, embeddings: np.ndarray, n: int) -> List[int]:
        """Select diverse sentences using clustering"""
        try:
            from sklearn.cluster import KMeans
            
            if len(embeddings) <= n:
                return list(range(len(embeddings)))
            
            # Cluster sentences
            kmeans = KMeans(n_clusters=n, random_state=42)
            cluster_labels = kmeans.fit_predict(embeddings)
            
            # Select sentence closest to each cluster center
            selected_indices = []
            for i in range(n):
                cluster_indices = np.where(cluster_labels == i)[0]
                if len(cluster_indices) > 0:
                    # Find sentence closest to cluster center
                    center = kmeans.cluster_centers_[i]
                    distances = np.linalg.norm(embeddings[cluster_indices] - center, axis=1)
                    closest_idx = cluster_indices[np.argmin(distances)]
                    selected_indices.append(int(closest_idx))
            
            return selected_indices
            
        except ImportError:
            # Fallback: select evenly spaced sentences
            step = max(1, len(embeddings) // n)
            return list(range(0, len(embeddings), step))[:n]
    
    def _calculate_cost(self, tokens: int, provider: str) -> float:
        """Calculate estimated cost"""
        costs = self.token_costs.get(provider, self.token_costs["openai"])
        return (tokens / 1000) * costs["input"]

class AgentCommunicationHub:
    """Centralized communication hub with token optimization"""
    
    def __init__(self):
        self.token_optimizer = TokenOptimizer()
        self.message_queues = defaultdict(Queue)
        self.active_agents = {}
        self.message_history = deque(maxlen=1000)
        self.routing_table = {}
        self.load_balancer = LoadBalancer()
    
    async def send_message(self, message: AgentMessage) -> bool:
        """Send message with token optimization"""
        try:
            # Compress content if needed
            if message.token_context.input_tokens == 0:
                compressed_content, token_context = self.token_optimizer.compress_content(message.content)
                message.compressed_content = compressed_content
                message.token_context = token_context
            
            # Route to best agent
            target_agent = self._route_message(message)
            message.to_agent = target_agent
            
            # Add to queue
            await self.message_queues[target_agent].put(message)
            self.message_history.append(message)
            
            logger.info(f"ðŸ“¨ Message routed: {message.from_agent} â†’ {target_agent} ({token_context.input_tokens} tokens)")
            return True
            
        except Exception as e:
            logger.error(f"Message send failed: {e}")
            return False
    
    def _route_message(self, message: AgentMessage) -> str:
        """Intelligent message routing"""
        # Check routing table first
        route_key = f"{message.from_agent}_{message.message_type.value}"
        if route_key in self.routing_table:
            return self.routing_table[route_key]
        
        # Use load balancer for optimal routing
        best_agent = self.load_balancer.get_best_agent(message.message_type)
        self.routing_table[route_key] = best_agent
        return best_agent
    
    async def get_messages(self, agent_id: str) -> List[AgentMessage]:
        """Get messages for specific agent"""
        messages = []
        queue = self.message_queues[agent_id]
        
        while not queue.empty():
            try:
                message = queue.get_nowait()
                messages.append(message)
            except:
                break
        
        return messages

class LoadBalancer:
    """Load balancer for agent distribution"""
    
    def __init__(self):
        self.agent_loads = defaultdict(int)
        self.agent_capabilities = {}
    
    def get_best_agent(self, message_type: MessageType) -> str:
        """Get best agent for message type"""
        # Simple round-robin for now
        # In production, would consider agent load, capabilities, and performance
        agents_for_type = self.agent_capabilities.get(message_type.value, [])
        if not agents_for_type:
            return "system_optimizer"  # Default fallback
        
        # Select agent with lowest load
        best_agent = min(agents_for_type, key=lambda a: self.agent_loads[a])
        self.agent_loads[best_agent] += 1
        return best_agent

# ============================================================================
# YOUTUBE LEARNING SYSTEM
# ============================================================================

class YouTubeLearningEngine:
    """Enhanced YouTube learning with transcription and knowledge integration"""
    
    def __init__(self):
        self.transcript_extractor = TranscriptExtractor()
        self.knowledge_integrator = KnowledgeIntegrator()
        self.learning_queue = Queue()
        self.learned_content = {}
        self.insight_cache = {}
        
    async def learn_from_video(self, video_url: str, learning_goals: List[str] = None) -> Dict[str, Any]:
        """Complete learning pipeline from YouTube video"""
        try:
            if not YOUTUBE_AVAILABLE:
                logger.warning("âš ï¸ YouTube libraries not available")
                return {"error": "YouTube libraries not installed"}
            
            logger.info(f"ðŸŽ¥ Starting YouTube learning: {video_url}")
            
            # Extract video ID
            video_id = self._extract_video_id(video_url)
            if not video_id:
                raise ValueError("Invalid YouTube URL")
            
            # Get transcript
            transcript_segments = await self.transcript_extractor.extract_transcript(video_id)
            if not transcript_segments:
                raise ValueError("No transcript available")
            
            # Analyze and extract insights
            insights = await self._extract_insights(transcript_segments, learning_goals)
            
            # Integrate knowledge
            integration_result = await self.knowledge_integrator.integrate_insights(
                video_id, insights, learning_goals
            )
            
            # Store learning results
            learning_result = {
                "video_id": video_id,
                "insights": [insight.__dict__ for insight in insights],
                "integration": integration_result,
                "timestamp": datetime.now().isoformat(),
                "learning_goals": learning_goals or []
            }
            
            self.learned_content[video_id] = learning_result
            
            logger.info(f"âœ… YouTube learning complete: {len(insights)} insights extracted")
            return learning_result
            
        except Exception as e:
            logger.error(f"YouTube learning failed: {e}")
            return {"error": str(e)}
    
    def _extract_video_id(self, url: str) -> Optional[str]:
        """Extract YouTube video ID from URL"""
        patterns = [
            r'(?:youtube\.com/watch\?v=|youtu\.be/)([^&\n?#]+)',
            r'youtube\.com/embed/([^?&\n]+)',
            r'youtube\.com/v/([^?&\n]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                video_id = match.group(1)
                if len(video_id) == 11:
                    return video_id
        
        return None
    
    async def _extract_insights(self, transcript_segments: List, learning_goals: List[str]) -> List[LearningInsight]:
        """Extract insights from transcript"""
        insights = []
        
        # Combine transcript text
        full_text = ' '.join([segment.text for segment in transcript_segments])
        
        # Use AI to extract insights (simplified for demo)
        # In production, would use actual LLM calls
        mock_insights = [
            LearningInsight(
                source="youtube_transcript",
                insight_type="concept",
                content="AI agents can significantly improve system efficiency",
                confidence=0.85,
                relevance_score=0.9,
                keywords=["ai", "agents", "efficiency", "automation"]
            ),
            LearningInsight(
                source="youtube_transcript", 
                insight_type="procedure",
                content="Implement token optimization to reduce costs",
                confidence=0.8,
                relevance_score=0.85,
                keywords=["tokens", "optimization", "cost", "efficiency"]
            )
        ]
        
        return mock_insights

class TranscriptExtractor:
    """YouTube transcript extraction"""
    
    async def extract_transcript(self, video_id: str) -> List:
        """Extract transcript from YouTube video"""
        try:
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            
            # Get English transcript or first available
            transcript = None
            try:
                transcript = transcript_list.find_transcript(['en'])
            except:
                transcript = list(transcript_list)[0]
            
            transcript_data = transcript.fetch()
            
            # Convert to segments
            segments = []
            for segment in transcript_data:
                segments.append(type('Segment', (), {
                    'start': segment['start'],
                    'duration': segment['duration'],
                    'text': segment['text']
                })())
            
            logger.info(f"âœ… Transcript extracted: {len(segments)} segments")
            return segments
            
        except Exception as e:
            logger.error(f"Transcript extraction failed: {e}")
            return []

class KnowledgeIntegrator:
    """Integrate learned knowledge into system"""
    
    def __init__(self):
        self.knowledge_graph = nx.DiGraph()
        self.integration_history = []
    
    async def integrate_insights(self, video_id: str, insights: List[LearningInsight], 
                               learning_goals: List[str]) -> Dict[str, Any]:
        """Integrate insights into knowledge base"""
        try:
            integration_result = {
                "integrations_created": 0,
                "knowledge_nodes_added": 0,
                "connections_made": 0
            }
            
            for insight in insights:
                # Add to knowledge graph
                self.knowledge_graph.add_node(insight.id, **insight.__dict__)
                self.knowledge_graph.add_edge(video_id, insight.id, relationship="contains_insight")
                
                integration_result["knowledge_nodes_added"] += 1
                integration_result["integrations_created"] += 1
            
            # Store integration history
            self.integration_history.append({
                "video_id": video_id,
                "insights_count": len(insights),
                "timestamp": datetime.now().isoformat()
            })
            
            logger.info(f"âœ… Knowledge integration complete: {len(insights)} insights integrated")
            return integration_result
            
        except Exception as e:
            logger.error(f"Knowledge integration failed: {e}")
            return {"error": str(e)}

# ============================================================================
# SELF-HEALING SYSTEM
# ============================================================================

class SelfHealingEngine:
    """Autonomous self-healing system"""
    
    def __init__(self):
        self.health_monitors = {}
        self.healing_actions = Queue()
        self.auto_fixes = {}
        self.healing_history = []
        self.system_health = 100.0
    
    async def start_monitoring(self):
        """Start continuous health monitoring"""
        while True:
            try:
                await self._check_system_health()
                await self._process_healing_actions()
                await asyncio.sleep(60)  # Check every minute
            except Exception as e:
                logger.error(f"Health monitoring error: {e}")
                await asyncio.sleep(30)
    
    async def _check_system_health(self):
        """Check overall system health"""
        health_checks = {
            "memory_usage": self._check_memory_usage(),
            "disk_space": self._check_disk_space(),
            "api_connectivity": self._check_api_connectivity(),
            "agent_status": self._check_agent_status(),
            "error_rates": self._check_error_rates()
        }
        
        # Calculate overall health
        health_score = sum(health_checks.values()) / len(health_checks)
        self.system_health = health_score
        
        if health_score < 80:
            await self._trigger_healing_actions(health_checks)
    
    def _check_memory_usage(self) -> float:
        """Check memory usage health"""
        try:
            import psutil
            memory = psutil.virtual_memory()
            usage_percent = memory.percent
            return max(0, 100 - usage_percent)
        except ImportError:
            return 90.0  # Assume healthy if psutil not available
    
    def _check_disk_space(self) -> float:
        """Check disk space health"""
        try:
            import psutil
            disk = psutil.disk_usage('/')
            usage_percent = (disk.used / disk.total) * 100
            return max(0, 100 - usage_percent)
        except ImportError:
            return 90.0
    
    def _check_api_connectivity(self) -> float:
        """Check API connectivity"""
        # Simplified check - would test actual API endpoints
        return 95.0
    
    def _check_agent_status(self) -> float:
        """Check agent health"""
        # Would check actual agent status
        return 90.0
    
    def _check_error_rates(self) -> float:
        """Check system error rates"""
        # Would monitor actual error logs
        return 85.0
    
    async def _trigger_healing_actions(self, health_checks: Dict[str, float]):
        """Trigger healing actions based on health issues"""
        for check_name, health_score in health_checks.items():
            if health_score < 70:
                healing_action = HealingAction(
                    issue_type=check_name,
                    severity=Priority.HIGH if health_score < 50 else Priority.MEDIUM,
                    description=f"Health issue detected in {check_name}: {health_score}%",
                    auto_fixable=True
                )
                
                await self.healing_actions.put(healing_action)
    
    async def _process_healing_actions(self):
        """Process pending healing actions"""
        while not self.healing_actions.empty():
            try:
                action = self.healing_actions.get_nowait()
                await self._execute_healing_action(action)
            except:
                break
    
    async def _execute_healing_action(self, action: HealingAction):
        """Execute a healing action"""
        try:
            logger.info(f"ðŸ”§ Executing healing action: {action.description}")
            
            if action.issue_type == "memory_usage":
                await self._heal_memory_usage(action)
            elif action.issue_type == "disk_space":
                await self._heal_disk_space(action)
            elif action.issue_type == "api_connectivity":
                await self._heal_api_connectivity(action)
            
            action.status = "completed"
            self.healing_history.append(action)
            
        except Exception as e:
            logger.error(f"Healing action failed: {e}")
            action.status = "failed"
    
    async def _heal_memory_usage(self, action: HealingAction):
        """Heal memory usage issues"""
        # Clear caches, restart memory-heavy processes
        import gc
        gc.collect()
        logger.info("ðŸ§¹ Memory cleanup completed")
    
    async def _heal_disk_space(self, action: HealingAction):
        """Heal disk space issues"""
        # Clean temporary files, rotate logs
        import tempfile
        import shutil
        
        temp_dir = tempfile.gettempdir()
        for item in os.listdir(temp_dir):
            try:
                item_path = os.path.join(temp_dir, item)
                if os.path.isfile(item_path):
                    os.remove(item_path)
            except:
                pass
        
        logger.info("ðŸ§¹ Disk cleanup completed")
    
    async def _heal_api_connectivity(self, action: HealingAction):
        """Heal API connectivity issues"""
        # Refresh API connections, rotate keys
        logger.info("ðŸ”„ API connections refreshed")

# ============================================================================
# GUARDRAIL SYSTEM
# ============================================================================

class GuardrailSystem:
    """Prevent hallucinations and ensure safe operations"""
    
    def __init__(self):
        self.content_filters = {}
        self.safety_checks = {}
        self.hallucination_detectors = {}
        self.violation_history = []
    
    async def validate_content(self, content: str, context: str = "") -> Dict[str, Any]:
        """Validate content for safety and accuracy"""
        validation_result = {
            "is_safe": True,
            "is_accurate": True,
            "confidence": 1.0,
            "violations": [],
            "warnings": []
        }
        
        # Check for hallucinations
        hallucination_check = await self._check_hallucinations(content, context)
        if not hallucination_check["passed"]:
            validation_result["is_accurate"] = False
            validation_result["violations"].append("potential_hallucination")
            validation_result["confidence"] *= 0.7
        
        # Check for harmful content
        safety_check = await self._check_safety(content)
        if not safety_check["passed"]:
            validation_result["is_safe"] = False
            validation_result["violations"].append("safety_concern")
            validation_result["confidence"] *= 0.5
        
        return validation_result
    
    async def _check_hallucinations(self, content: str, context: str) -> Dict[str, Any]:
        """Check for potential hallucinations"""
        # Simplified check - would use actual fact-checking
        red_flags = [
            "I believe", "I think", "probably", "might be", "could be",
            "according to", "it seems", "appears to be"
        ]
        
        hallucination_score = 0
        for flag in red_flags:
            if flag.lower() in content.lower():
                hallucination_score += 1
        
        passed = hallucination_score < 3  # Allow some uncertainty
        
        return {
            "passed": passed,
            "score": hallucination_score,
            "flags_found": hallucination_score
        }
    
    async def _check_safety(self, content: str) -> Dict[str, Any]:
        """Check for harmful or inappropriate content"""
        # Simplified safety check
        harmful_patterns = [
            "hack", "exploit", "malicious", "harmful", "illegal"
        ]
        
        safety_score = 0
        for pattern in harmful_patterns:
            if pattern.lower() in content.lower():
                safety_score += 1
        
        passed = safety_score == 0
        
        return {
            "passed": passed,
            "score": safety_score,
            "patterns_found": safety_score
        }

# ============================================================================
# MAIN ENHANCED AGENT SYSTEM
# ============================================================================

class EnhancedMultiAgentSystem:
    """Main enhanced multi-agent system"""
    
    def __init__(self):
        self.communication_hub = AgentCommunicationHub()
        self.youtube_learner = YouTubeLearningEngine()
        self.self_healer = SelfHealingEngine()
        self.guardrails = GuardrailSystem()
        self.agents = {}
        self.is_running = False
        self.system_metrics = {}
        
        # Initialize context management
        self.context_manager = ContextWindowManager(
            max_context_window=4096,  # Adjust based on model
            warning_threshold=0.75
        )
        
    async def initialize(self):
        """Initialize the enhanced system"""
        logger.info("ðŸš€ Initializing Enhanced Multi-Agent System...")
        
        # Initialize components
        await self._initialize_agents()
        await self._setup_integrations()
        
        logger.info("âœ… Enhanced Multi-Agent System initialized")
    
    async def _initialize_agents(self):
        """Initialize all specialized agents"""
        agent_configs = [
            PydanticAgentConfig(
                agent_id="system_optimizer",
                role=AgentRole.SYSTEM_OPTIMIZER,
                capabilities=["performance_analysis", "optimization", "metrics"]
            ),
            PydanticAgentConfig(
                agent_id="youtube_learner",
                role=AgentRole.YOUTUBE_LEARNER,
                capabilities=["transcription", "learning", "knowledge_integration"]
            ),
            PydanticAgentConfig(
                agent_id="self_healer",
                role=AgentRole.SELF_HEALER,
                capabilities=["health_monitoring", "auto_repair", "recovery"]
            ),
            PydanticAgentConfig(
                agent_id="guardrail",
                role=AgentRole.GUARDRAIL,
                capabilities=["content_validation", "safety_checks", "hallucination_detection"]
            ),
            PydanticAgentConfig(
                agent_id="token_optimizer",
                role=AgentRole.TOKEN_OPTIMIZER,
                capabilities=["token_optimization", "cost_reduction", "compression"]
            )
        ]
        
        for config in agent_configs:
            agent = EnhancedAgent(config, self.communication_hub)
            self.agents[config.agent_id] = agent
            logger.info(f"âœ… Agent initialized: {config.agent_id}")
    
    async def _setup_integrations(self):
        """Setup integrations with open-source tools"""
        if OPENCLAW_AVAILABLE:
            logger.info("âœ… OpenCLAW integration available")
        
        if AGENT_ZERO_AVAILABLE:
            logger.info("âœ… Agent Zero integration available")
        
        if N8N_AVAILABLE:
            logger.info("âœ… N8N workflow integration available")
        
        if CHROMA_AVAILABLE:
            logger.info("âœ… ChromaDB vector storage available")
    
    async def start(self):
        """Start the enhanced system"""
        self.is_running = True
        
        logger.info("ðŸš€ Starting Enhanced Multi-Agent System...")
        
        # Start all agents
        agent_tasks = []
        for agent in self.agents.values():
            agent_tasks.append(agent.start())
        
        # Start background services
        background_tasks = [
            self.self_healer.start_monitoring(),
            self._system_metrics_loop(),
            self._continuous_learning_loop()
        ]
        
        # Run all tasks
        await gather(*agent_tasks, *background_tasks)
    
    async def _system_metrics_loop(self):
        """Collect system metrics continuously"""
        while self.is_running:
            try:
                # Check context usage before adding metrics
                context_check = self.context_manager.add_tokens(
                    50,  # Estimated tokens for metrics
                    "system_metrics",
                    f"Collecting system metrics at {datetime.now()}"
                )
                
                # Handle context handoff if needed
                if context_check:
                    logger.warning("ðŸ”„ Context handoff triggered during metrics collection")
                    await self._handle_context_handoff(context_check)
                
                self.system_metrics = {
                    "timestamp": datetime.now().isoformat(),
                    "active_agents": len([a for a in self.agents.values() if a.is_active]),
                    "system_health": self.self_healer.system_health,
                    "total_messages": len(self.communication_hub.message_history),
                    "learned_videos": len(self.youtube_learner.learned_content),
                    "context_usage": self.context_manager.get_status_report()
                }
                
                await asyncio.sleep(300)  # Every 5 minutes
            except Exception as e:
                logger.error(f"Metrics collection error: {e}")
                await asyncio.sleep(60)
    
    async def _continuous_learning_loop(self):
        """Continuous learning from YouTube"""
        while self.is_running:
            try:
                # Check context before learning
                context_check = self.context_manager.add_tokens(
                    30,  # Estimated tokens for learning cycle
                    "youtube_learning",
                    "YouTube learning cycle initiated"
                )
                
                if context_check:
                    logger.info("ðŸ”„ Context handoff during YouTube learning")
                    await self._handle_context_handoff(context_check)
                
                # Would fetch relevant videos from subscriptions or recommendations
                # For demo, just wait
                await asyncio.sleep(3600)  # Every hour
            except Exception as e:
                logger.error(f"Learning loop error: {e}")
                await asyncio.sleep(300)
    
    async def _handle_context_handoff(self, handoff_package):
        """Handle context handoff when window is full"""
        logger.info(f"ðŸ”„ Processing context handoff: {handoff_package.session_id}")
        
        # Save handoff to file for persistence
        handoff_file = Path(f"generated_content/context_handoffs/{handoff_package.session_id}.json")
        handoff_file.parent.mkdir(parents=True, exist_ok=True)
        
        handoff_data = {
            "session_id": handoff_package.session_id,
            "handoff_timestamp": handoff_package.handoff_timestamp.isoformat(),
            "original_tokens": handoff_package.original_context_tokens,
            "compressed_tokens": handoff_package.compressed_tokens,
            "compression_ratio": handoff_package.compression_ratio,
            "priority_level": handoff_package.priority_level,
            "project_goal": handoff_package.project_goal,
            "current_phase": handoff_package.current_phase,
            "key_decisions": handoff_package.key_decisions,
            "immediate_actions": handoff_package.immediate_actions,
            "blockers_issues": handoff_package.blockers_issues,
            "files_created": handoff_package.files_created,
            "dependencies_needed": handoff_package.dependencies_needed,
            "code_status": handoff_package.code_status,
            "next_agent_focus": handoff_package.next_agent_focus,
            "continuation_plan": handoff_package.continuation_plan,
            "success_criteria": handoff_package.success_criteria
        }
        
        handoff_file.write_text(json.dumps(handoff_data, indent=2))
        
        # Reset context for fresh start
        self.context_manager.reset_context()
        
        # Log key information from handoff
        logger.info(f"ðŸ“‹ Handoff Summary:")
        logger.info(f"   Goal: {handoff_package.project_goal}")
        logger.info(f"   Phase: {handoff_package.current_phase}")
        logger.info(f"   Next Focus: {handoff_package.next_agent_focus}")
        logger.info(f"   Files Created: {len(handoff_package.files_created)}")
        logger.info(f"   Issues: {len(handoff_package.blockers_issues)}")
        
        # In production, would pass handoff to next agent or instance
        # For now, just log and continue

class EnhancedAgent:
    """Enhanced individual agent"""
    
    def __init__(self, config: PydanticAgentConfig, communication_hub: AgentCommunicationHub):
        self.config = config
        self.communication_hub = communication_hub
        self.is_active = False
        self.message_queue = Queue()
        self.memory = {}
        self.learning_enabled = config.learning_enabled
    
    async def start(self):
        """Start agent operation"""
        self.is_active = True
        logger.info(f"ðŸ¤– Agent {self.config.agent_id} started")
        
        while self.is_active:
            try:
                # Get messages
                messages = await self.communication_hub.get_messages(self.config.agent_id)
                
                # Process messages
                for message in messages:
                    await self._process_message(message)
                
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"Agent {self.config.agent_id} error: {e}")
                await asyncio.sleep(5)
    
    async def _process_message(self, message: AgentMessage):
        """Process incoming message"""
        try:
            logger.info(f"ðŸ“¨ {self.config.agent_id} processing: {message.message_type.value}")
            
            # Decompress content if needed
            if message.compressed_content:
                try:
                    message.content = json.loads(message.compressed_content)
                except:
                    pass
            
            # Handle based on message type
            if message.message_type == MessageType.REQUEST:
                await self._handle_request(message)
            elif message.message_type == MessageType.LEARNING:
                await self._handle_learning(message)
            elif message.message_type == MessageType.HEALING:
                await self._handle_healing(message)
            
        except Exception as e:
            logger.error(f"Message processing error: {e}")
    
    async def _handle_request(self, message: AgentMessage):
        """Handle request message"""
        # Process based on agent role
        if self.config.role == AgentRole.YOUTUBE_LEARNER:
            await self._handle_youtube_learning(message)
        elif self.config.role == AgentRole.SELF_HEALER:
            await self._handle_self_healing(message)
        elif self.config.role == AgentRole.GUARDRAIL:
            await self._handle_guardrail(message)
        elif self.config.role == AgentRole.TOKEN_OPTIMIZER:
            await self._handle_token_optimization(message)
    
    async def _handle_youtube_learning(self, message: AgentMessage):
        """Handle YouTube learning requests"""
        video_url = message.content.get("video_url")
        learning_goals = message.content.get("learning_goals", [])
        
        if video_url:
            result = await self.communication_hub.youtube_learner.learn_from_video(video_url, learning_goals)
            
            # Send response
            response = AgentMessage(
                from_agent=self.config.agent_id,
                to_agent=message.from_agent,
                message_type=MessageType.RESPONSE,
                content={"result": result},
                priority=message.priority
            )
            
            await self.communication_hub.send_message(response)
    
    async def _handle_self_healing(self, message: AgentMessage):
        """Handle self-healing requests"""
        # Process healing action
        healing_action = HealingAction(**message.content)
        await self.communication_hub.self_healer._execute_healing_action(healing_action)
    
    async def _handle_guardrail(self, message: AgentMessage):
        """Handle guardrail checks"""
        content = message.content.get("content", "")
        context = message.content.get("context", "")
        
        validation_result = await self.communication_hub.guardrails.validate_content(content, context)
        
        response = AgentMessage(
            from_agent=self.config.agent_id,
            to_agent=message.from_agent,
            message_type=MessageType.RESPONSE,
            content={"validation": validation_result},
            priority=message.priority
        )
        
        await self.communication_hub.send_message(response)
    
    async def _handle_token_optimization(self, message: AgentMessage):
        """Handle token optimization requests"""
        content = message.content.get("content", {})
        
        compressed_content, token_context = self.communication_hub.token_optimizer.compress_content(content)
        
        response = AgentMessage(
            from_agent=self.config.agent_id,
            to_agent=message.from_agent,
            message_type=MessageType.RESPONSE,
            content={
                "compressed_content": compressed_content,
                "token_context": token_context.__dict__
            },
            priority=message.priority
        )
        
        await self.communication_hub.send_message(response)

# ============================================================================
# MAIN ENTRY POINT
# ============================================================================

async def main():
    """Main entry point"""
    system = EnhancedMultiAgentSystem()
    await system.initialize()
    await system.start()

if __name__ == "__main__":
    asyncio.run(main())
